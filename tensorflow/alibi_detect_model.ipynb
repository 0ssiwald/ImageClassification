{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oswal\\anaconda3\\envs\\py310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/simplifing-image-outlier-detection-with-alibi-detect-6aea686bf7ba\n",
    "\n",
    "# GPU https://www.youtube.com/watch?v=NrJz3ACosJA&t=6s conda activate py310\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D,\\\n",
    "    Dense, Layer, Reshape, InputLayer, Flatten, Input, MaxPooling2D\n",
    "from alibi_detect.od import OutlierAE\n",
    "from alibi_detect.utils.visualize import plot_instance_score, plot_feature_outlier_image\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare image data \n",
    "def img_to_np(path, resize = True, extract_labels=False):  \n",
    "    img_array = []\n",
    "    labels = []\n",
    "    fpaths = glob.glob(path, recursive=True)\n",
    "    for fname in fpaths:\n",
    "        if(extract_labels): \n",
    "            if '_bad' in os.path.basename(fname):\n",
    "                labels.append(1)  # 1 for outlier\n",
    "            else:\n",
    "                labels.append(0)  # 0 for non-outlier\n",
    "        img = Image.open(fname).convert(\"L\") # Grayscale when using \"RGB\" you have to change the encoder and decoder \n",
    "        if(resize): img = img.resize((64,64))\n",
    "        img_array.append(np.asarray(img))\n",
    "    images = np.array(img_array)\n",
    "    if(extract_labels): return images, np.array(labels)\n",
    "    return images\n",
    "\n",
    "path_train = r'C:\\Users\\Ossi\\Desktop\\ImageClassification\\elpv-dataset\\train_without_bad_images\\**\\*'\n",
    "path_test = r'C:\\Users\\Ossi\\Desktop\\ImageClassification\\elpv-dataset\\test_images\\**\\*'\n",
    "path_treshold = r'C:\\Users\\Ossi\\Desktop\\ImageClassification\\elpv-dataset\\train_without_good_images\\**\\*'\n",
    "\n",
    "train = img_to_np(path_train)\n",
    "test, test_labels = img_to_np(path_test, extract_labels=True)\n",
    "set_bad_threshold = img_to_np(path_treshold)\n",
    "train = train.astype('float32') / 255.0\n",
    "test = test.astype('float32') / 255.0\n",
    "set_bad_threshold  = set_bad_threshold.astype('float32') / 255.0\n",
    "# Reshape to include the channel dimension -> needed with grayscale conversion\n",
    "train = np.expand_dims(train, axis=-1)\n",
    "test = np.expand_dims(test, axis=-1)\n",
    "set_bad_threshold = np.expand_dims(set_bad_threshold, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [=] - 5s 59ms/step - loss_ma: 0.0372\n",
      "15/15 [=] - 0s 29ms/step - loss_ma: 0.0253\n",
      "15/15 [=] - 0s 29ms/step - loss_ma: 0.0128\n",
      "15/15 [=] - 0s 28ms/step - loss_ma: 0.0111\n",
      "15/15 [=] - 0s 28ms/step - loss_ma: 0.0105\n",
      "15/15 [=] - 0s 28ms/step - loss_ma: 0.0105\n",
      "15/15 [=] - 0s 28ms/step - loss_ma: 0.0103\n",
      "15/15 [=] - 0s 28ms/step - loss_ma: 0.0100\n",
      "15/15 [=] - 0s 28ms/step - loss_ma: 0.0096\n",
      "15/15 [=] - 0s 28ms/step - loss_ma: 0.0099\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "encoding_dim = 1024\n",
    "dense_dim = [8, 8, 128]\n",
    "\n",
    "# Define the encoder\n",
    "encoder_net = tf.keras.Sequential([\n",
    "    Input(shape=(64, 64, 1)),  # Updated input shape for grayscale images\n",
    "    Conv2D(64, 4, strides=2, padding='same', activation='relu'),\n",
    "    Conv2D(128, 4, strides=2, padding='same', activation='relu'),\n",
    "    Conv2D(512, 4, strides=2, padding='same', activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(encoding_dim)\n",
    "])\n",
    "\n",
    "# Define the decoder\n",
    "decoder_net = tf.keras.Sequential([\n",
    "    Input(shape=(encoding_dim,)),\n",
    "    Dense(np.prod(dense_dim)),\n",
    "    Reshape(target_shape=dense_dim),\n",
    "    Conv2DTranspose(256, 4, strides=2, padding='same', activation='relu'),\n",
    "    Conv2DTranspose(64, 4, strides=2, padding='same', activation='relu'),\n",
    "    Conv2DTranspose(1, 4, strides=2, padding='same', activation='sigmoid')  # Updated output channels to 1\n",
    "])\n",
    "\n",
    "# Create the OutlierAE model\n",
    "od = OutlierAE(\n",
    "    threshold=0.001,\n",
    "    encoder_net=encoder_net,\n",
    "    decoder_net=decoder_net\n",
    ")\n",
    "\n",
    "# Compile and train the model\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "od.fit(train, epochs=10, verbose=True, optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The first argument to `Layer.call` must always be passed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mae\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ossi\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Ossi\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\utils\\layer_utils.py:812\u001b[0m, in \u001b[0;36mCallFunctionSpec.split_out_first_arg\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    810\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arg_names[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe first argument to `Layer.call` must always be passed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    814\u001b[0m     )\n\u001b[0;32m    815\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs, args, kwargs\n",
      "\u001b[1;31mValueError\u001b[0m: The first argument to `Layer.call` must always be passed."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 52.98%\n"
     ]
    }
   ],
   "source": [
    "# Infer threshold on test set\n",
    "# I think setting a threshold with outlier pictures with 95 meaning 95% of outliers are classified as such makes sense ????\n",
    "od.infer_threshold(set_bad_threshold, threshold_perc=95)\n",
    "\n",
    "# Test the model on the test set\n",
    "preds = od.predict(test, outlier_type='instance',\n",
    "                   return_instance_score=True,\n",
    "                   return_feature_score=True)\n",
    "\n",
    "predicted_labels = preds['data']['is_outlier']\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
