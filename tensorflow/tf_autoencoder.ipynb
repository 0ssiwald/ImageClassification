{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/tutorials/generative/autoencoder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "from log_training import TrainingLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare image data \n",
    "def img_to_np(path, resize = True, extract_labels=False):  \n",
    "    img_array = []\n",
    "    labels = []\n",
    "    fpaths = glob.glob(path, recursive=True)\n",
    "    for fname in fpaths:\n",
    "        if(extract_labels): \n",
    "            if '_bad' in os.path.basename(fname):\n",
    "                labels.append(1)  # 1 for outlier\n",
    "            else:\n",
    "                labels.append(0)  # 0 for non-outlier\n",
    "        img = Image.open(fname).convert(\"L\") # Grayscale when using \"RGB\" you have to change the encoder and decoder \n",
    "        if(resize): img = img.resize((64,64))\n",
    "        img_array.append(np.asarray(img))\n",
    "    images = np.array(img_array)\n",
    "    if(extract_labels): return images, np.array(labels)\n",
    "    return images\n",
    "\n",
    "path_train = r'C:\\Users\\oswal\\Desktop\\ImageClassification\\elpv-dataset\\train_without_bad_images\\**\\*'\n",
    "path_test = r'C:\\Users\\oswal\\Desktop\\ImageClassification\\elpv-dataset\\test_images\\**\\*'\n",
    "path_treshold = r'C:\\Users\\oswal\\Desktop\\ImageClassification\\elpv-dataset\\train_without_good_images\\**\\*'\n",
    "\n",
    "train = img_to_np(path_train)\n",
    "test, test_labels = img_to_np(path_test, extract_labels=True)\n",
    "set_bad_threshold = img_to_np(path_treshold)\n",
    "train = train.astype('float32') / 255.0\n",
    "test = test.astype('float32') / 255.0\n",
    "set_bad_threshold  = set_bad_threshold.astype('float32') / 255.0\n",
    "# Reshape to include the channel dimension -> needed with grayscale conversion\n",
    "train = np.expand_dims(train, axis=-1)\n",
    "test = np.expand_dims(test, axis=-1)\n",
    "set_bad_threshold = np.expand_dims(set_bad_threshold, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=10\n",
    "LEARNING_RATE=1e-4\n",
    "BATCH_SIZE=32\n",
    "OPTIMIZER='adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "24/24 [==============================] - 36s 34ms/step - loss: 0.0329 - val_loss: 0.0107\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - 0s 15ms/step - loss: 0.0147 - val_loss: 0.0063\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.0119 - val_loss: 0.0060\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - 0s 15ms/step - loss: 0.0113 - val_loss: 0.0062\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.0108 - val_loss: 0.0064\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - 0s 15ms/step - loss: 0.0107 - val_loss: 0.0072\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.0108 - val_loss: 0.0069\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - 0s 15ms/step - loss: 0.0106 - val_loss: 0.0068\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - 0s 15ms/step - loss: 0.0102 - val_loss: 0.0071\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - 0s 15ms/step - loss: 0.0099 - val_loss: 0.0061\n"
     ]
    }
   ],
   "source": [
    "# Custom Autoencoder model\n",
    "class Autoencoder(tf.keras.Model):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    # For logging\n",
    "    def summary(self):\n",
    "        return self.encoder.summary(), self.decoder.summary()\n",
    "\n",
    "# Model parameters\n",
    "encoding_dim = 1024\n",
    "dense_dim = [8, 8, 128]\n",
    "\n",
    "# Define the encoder\n",
    "encoder = tf.keras.Sequential([\n",
    "    layers.Input(shape=(64, 64, 1)),  # 64x64 pixels with one color channel (gray)\n",
    "     # extracts 64 feature maps using 4x4 filters | output dimensions are reduced by half due to the stride of 2.\n",
    "    layers.Conv2D(64, 4, strides=2, padding='same', activation='relu'),# (64, 64, 1) -> (32, 32, 64)\n",
    "    layers.Conv2D(128, 4, strides=2, padding='same', activation='relu'), # extracts 64 feature maps using 4x4 filters (32, 32, 64) -> (16, 16, 128)\n",
    "    layers.Conv2D(512, 4, strides=2, padding='same', activation='relu'), # (16, 16, 128) -> (8, 8, 512)\n",
    "    layers.Flatten(), # flattens the 3D tensor (8, 8, 512) into a 1D vector of size 32768\n",
    "    layers.Dense(encoding_dim) # fully connected layer reduces the flattened vector to a 1D vector of size 1024\n",
    "])\n",
    "\n",
    "# Define the decoder\n",
    "decoder = tf.keras.Sequential([\n",
    "    layers.Input(shape=(encoding_dim,)),\n",
    "    layers.Dense(np.prod(dense_dim)), # takes 1024-dimensional vector and maps it to size 8192, which corresponds to the flattened form of the next target shape (8, 8, 128)\n",
    "    layers.Reshape(target_shape=dense_dim), # reshapes 8192 into (8, 8, 128)\n",
    "    layers.Conv2DTranspose(256, 4, strides=2, padding='same', activation='relu'), # deconvolution -> increases the depth to 256 feature maps (8, 8, 128) -> (16, 16, 256)\n",
    "    layers.Conv2DTranspose(64, 4, strides=2, padding='same', activation='relu'), # (16, 16, 256) -> (32, 32, 64)\n",
    "    layers.Conv2DTranspose(1, 4, strides=2, padding='same', activation='sigmoid')  # (32, 32, 64) -> (64, 64, 1)\n",
    "])\n",
    "\n",
    "# Create the Autoencoder model\n",
    "autoencoder = Autoencoder(encoder, decoder)\n",
    "# loss function is Mean Squared Error(MSE)\n",
    "autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss='mse')\n",
    "\n",
    "# Train the model\n",
    "# not shure if validation_split is best but validation_data I dont understand completely \n",
    "history = autoencoder.fit(train, train, epochs=EPOCHS, batch_size=BATCH_SIZE, shuffle=True, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 12ms/step\n",
      "Accuracy: 60.71%\n"
     ]
    }
   ],
   "source": [
    "# Compute the reconstruction error for the test set\n",
    "reconstructions = autoencoder.predict(test)\n",
    "reconstruction_errors = np.mean(np.square(test - reconstructions), axis=(1, 2, 3))\n",
    "\n",
    "# Set a threshold for classifying outliers \n",
    "# 90 means 90% are classified as inliners | because test set is 50% outliers 50 makes sense I think\n",
    "# maybe make testset with only outliers and set a threshold of 95 for example \n",
    "threshold = np.percentile(reconstruction_errors, 50)  \n",
    "\n",
    "# Predict whether each test instance is an outlier\n",
    "predicted_labels = (reconstruction_errors > threshold).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"epochs\": 10,\n",
      "\"learning_rate\": 0.0001,\n",
      "\"batch_size\": 32,\n",
      "\"optimizer\": adam,\n",
      "\"train_accuracy\": None,\n",
      "\"train_val_accuracy\": None,\n",
      "\"train_loss\": [0.03285584971308708, 0.014688660390675068, 0.01194201409816742, 0.011253530159592628, 0.010828845202922821, 0.010693410411477089, 0.010759016498923302, 0.010642633773386478, 0.010185929015278816, 0.009922865778207779],\n",
      "\"train_val_loss\": [0.010713508352637291, 0.006333703175187111, 0.005967853125184774, 0.006221625488251448, 0.006382073741406202, 0.00719117047265172, 0.006908770184963942, 0.006842107977718115, 0.007095559500157833, 0.0061037871055305],\n",
      "\"test_accuracy\": 0.6071428571428571,\n",
      "\"test_loss\": None,\n",
      "\"timestamp\": 2024-08-28 17:03:37, \n",
      "\"model_summary\": Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 32, 32, 64)        1088      \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 16, 16, 128)       131200    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 8, 8, 512)         1049088   \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 32768)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1024)              33555456  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,736,832\n",
      "Trainable params: 34,736,832\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 8192)              8396800   \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2DT  (None, 16, 16, 256)      524544    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_4 (Conv2DT  (None, 32, 32, 64)       262208    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_5 (Conv2DT  (None, 64, 64, 1)        1025      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,184,577\n",
      "Trainable params: 9,184,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      ",\n",
      "\"trials_data\": \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Log all the data and store it\n",
    "logger = TrainingLogger(epochs=EPOCHS, learning_rate=LEARNING_RATE, batch_size=BATCH_SIZE, optimizer=OPTIMIZER)\n",
    "logger.capture_model_summary(autoencoder)\n",
    "logger.update_train_metrics(val_accuracy=None, val_loss=history.history['val_loss'], accuracy=None, loss=history.history['loss'])\n",
    "logger.update_test_metrics(accuracy=accuracy, loss=None)\n",
    "logger.print_and_save_log()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
